
% Script for Logistic Regression model
% CCTA data analysis
% Created 07-Apr-2018 Kevin M. Johnson, M.D. Yale University
% Revised 04-Jan-2019

% Run Logistic Regression model for each outcome group

%Three outcomes were defined: all deaths, coronary artery disease deaths,
%and CHD deaths plus myocardial infarctions. The variable numbering 1
%through 3 generallly refers to these three outcomes.

%Inputs: CCTAtables are generated by the B_threshold_inputs.m function.
%Outputs: scores_LogR1 are the linear discriminant scores
%         X_LogR1 and Y_LogR1 define the ROC curve for all deaths
%         AUC_LogR1 is a 1x3 vector containing the AUCs, followed by the
%         pointwise bootstrap confidence interval upper and lower bounds

%This routine is time consuming. To run much faster, disable bootstrapping. 
%No confidence intervals will be returned in that case.
    bootstrapping='off'; %'on' or 'off'
    
%load data
    load input_data/CCTAtable1.mat
    load input_data/CCTAtable2.mat
    load input_data/CCTAtable3.mat

%all deaths
    [response_LogR1,scores_LogR1,X_LogR1,Y_LogR1,T_LogR1,AUC_LogR1] = func_LogR(CCTAtable1,bootstrapping);
   
%CHD deaths
    [response_LogR2,scores_LogR2,X_LogR2,Y_LogR2,T_LogR2,AUC_LogR2] = func_LogR(CCTAtable2,bootstrapping);
  
%CHD deaths + MI
    [response_LogR3,scores_LogR3,X_LogR3,Y_LogR3,T_LogR3,AUC_LogR3] = func_LogR(CCTAtable3,bootstrapping);
     
% Make results table
    LogisticR_results_table=table(AUC_LogR1',AUC_LogR2',AUC_LogR3');
    LogisticR_results_table.Properties.VariableNames{'Var1'}='LogR_AUC_alldeaths';
    LogisticR_results_table.Properties.VariableNames{'Var2'}='LogR_AUC_CHDdeaths';
    LogisticR_results_table.Properties.VariableNames{'Var3'}='LogR_AUC_CHDdeathsplusMI';
    LogisticR_results_table.Properties.VariableDescriptions={'AUC followed by CI','AUC followed by CI','AUC followed by CI'};
    LogisticR_results_table.Properties.UserData.percentile=parameter.percentile;
    disp(LogisticR_results_table)
    
% Save
    save('results/LogisticR_results_table','LogisticR_results_table')
    writetable(LogisticR_results_table,'results/LogisticR_results_table')
    

function [response,scores,X_LogR,Y_LogR,T_LogR,AUC_LogR] = func_LogR(trainingData,bootstrapping)

% Extract predictors and response
    inputTable = trainingData;
    warning('off','stats:glmfit:IterationLimit')

% Split matrices in the input table into vectors
    predictorNames =inputTable.Properties.VariableNames(1:end-1);
    predictors = inputTable(:, predictorNames);
    outcomeName =inputTable.Properties.VariableNames(end);
    response = table2array(inputTable(:, outcomeName));

% Train a classifier
    successClass = '1';
    failureClass = '0';

% Compute the majority response class
    numSuccess = sum(response == successClass);
    numFailure = sum(response == failureClass);
    if numSuccess > numFailure
        missingClass = successClass;
    else
        missingClass = failureClass;
    end
    responseCategories = {successClass, failureClass};
    successFailureAndMissingClasses = categorical({successClass; failureClass; missingClass}, responseCategories);
    isMissing = isundefined(response);
    zeroOneResponse = double(ismember(response, successClass));
    zeroOneResponse(isMissing) = NaN;

% Prepare input arguments to fitglm.
    concatenatedPredictorsAndResponse = [predictors, table(zeroOneResponse)];
    
% Train using fitglm.
    GeneralizedLinearModel = fitglm(concatenatedPredictorsAndResponse,...
        'Distribution', 'binomial','link', 'logit');

% Convert predicted probabilities to predicted class labels and scores.
    convertSuccessProbsToPredictions = @(p) successFailureAndMissingClasses( ~isnan(p).*( (p<0.5) + 1 ) + isnan(p)*3 );
    returnMultipleValuesFcn = @(varargin) varargin{1:max(1,nargout)};
    scoresFcn = @(p) [p, 1-p];
    predictionsAndScoresFcn = @(p) returnMultipleValuesFcn( convertSuccessProbsToPredictions(p), scoresFcn(p) );

% Create the result struct with predict function
    predictorExtractionFcn = @(t) t(:, predictorNames);
    logisticRegressionPredictFcn = @(x) predictionsAndScoresFcn( predict(GeneralizedLinearModel, x) );
    trainedClassifier.predictFcn = @(x) logisticRegressionPredictFcn(predictorExtractionFcn(x));

% Add additional fields to the result struct
    trainedClassifier.RequiredVariables = {'Var1', 'Var2', 'Var3', 'Var4', 'Var5', 'Var6', 'Var7', 'Var8', 'Var9', 'Var10', 'Var11', 'Var12', 'Var13', 'Var14', 'Var15', 'Var16', 'Var17', 'Var18', 'Var19', 'Var20', 'Var21', 'Var22', 'Var23', 'Var24', 'Var25', 'Var26', 'Var27', 'Var28', 'Var29', 'Var30', 'Var31', 'Var32', 'Var33', 'Var34', 'Var35', 'Var36', 'Var37', 'Var38', 'Var39', 'Var40', 'Var41', 'Var42', 'Var43', 'Var44', 'Var45', 'Var46', 'Var47', 'Var48'};
    trainedClassifier.GeneralizedLinearModel = GeneralizedLinearModel;
    trainedClassifier.SuccessClass = successClass;
    trainedClassifier.FailureClass = failureClass;
    trainedClassifier.MissingClass = missingClass;
    trainedClassifier.ClassNames = {successClass; failureClass};
    trainedClassifier.About = 'This struct is a trained model exported from Classification Learner R2017b.';
    trainedClassifier.HowToPredict = sprintf('To make predictions on a new table, T, use: \n  yfit = c.predictFcn(T) \nreplacing ''c'' with the name of the variable that is this struct, e.g. ''trainedModel''. \n \nThe table, T, must contain the variables returned by: \n  c.RequiredVariables \nVariable formats (e.g. matrix/vector, datatype) must match the original training data. \nAdditional variables are ignored. \n \nFor more information, see <a href="matlab:helpview(fullfile(docroot, ''stats'', ''stats.map''), ''appclassification_exportmodeltoworkspace'')">How to predict using an exported model</a>.');

% Extract predictors and response
    predictors = inputTable(:, predictorNames);
    response = table2array(inputTable(:, outcomeName));
    isCategoricalPredictor = [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false];

% Perform cross-validation
    KFolds = 10;
    cvp = cvpartition(response, 'KFold', KFolds);
    
% Initialize the predictions to the proper sizes
    validationPredictions = response;
    numObservations = size(predictors, 1);
    numClasses = 2;
    validationScores = NaN(numObservations, numClasses);
    for fold = 1:KFolds
        trainingPredictors = predictors(cvp.training(fold), :);
        trainingResponse = response(cvp.training(fold), :);

        % Train a classifier
        successClass = '1';
        failureClass = '0';
        
        % Compute the majority response class
        numSuccess = sum(trainingResponse == successClass);
        numFailure = sum(trainingResponse == failureClass);
        if numSuccess > numFailure
            missingClass = successClass;
        else
            missingClass = failureClass;
        end
        responseCategories = {successClass, failureClass};
        successFailureAndMissingClasses = categorical({successClass; failureClass; missingClass}, responseCategories);
        isMissing = isundefined(trainingResponse);
        zeroOneResponse = double(ismember(trainingResponse, successClass));
        zeroOneResponse(isMissing) = NaN;
        
        % Prepare input arguments to fitglm.
        concatenatedPredictorsAndResponse = [trainingPredictors, table(zeroOneResponse)];
        
        % Train using fitglm.
        GeneralizedLinearModel = fitglm(...
            concatenatedPredictorsAndResponse, ...
            'Distribution', 'binomial', ...
            'link', 'logit');

        % Convert predicted probabilities to predicted class labels and scores.
        convertSuccessProbsToPredictions = @(p) successFailureAndMissingClasses( ~isnan(p).*( (p<0.5) + 1 ) + isnan(p)*3 );
        returnMultipleValuesFcn = @(varargin) varargin{1:max(1,nargout)};
        scoresFcn = @(p) [p, 1-p];
        predictionsAndScoresFcn = @(p) returnMultipleValuesFcn( convertSuccessProbsToPredictions(p), scoresFcn(p) );

        % Create the result struct with predict function
        logisticRegressionPredictFcn = @(x) predictionsAndScoresFcn( predict(GeneralizedLinearModel, x) );
        validationPredictFcn = @(x) logisticRegressionPredictFcn(x);

        % Compute validation predictions
        validationPredictors = predictors(cvp.test(fold), :);
        [foldPredictions, foldScores] = validationPredictFcn(validationPredictors);

        % Store predictions in the original order
        validationPredictions(cvp.test(fold), :) = foldPredictions;
        validationScores(cvp.test(fold), :) = foldScores;
    end
    scores=validationScores(:,1);
    
    % ROC curves with bootstrap confidence intervals
        if strcmp(bootstrapping,'on')
            [X_LogR,Y_LogR,T_LogR,AUC_LogR]=perfcurve(response,scores,1,'nboot',1000);
        else
            [X_LogR,Y_LogR,T_LogR,AUC_LogR]=perfcurve(response,scores,1);
        end
        
end