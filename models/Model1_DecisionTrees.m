
% Script for Decision Tree model
% CCTA data analysis
% Created 07-Apr-2018 Kevin M. Johnson, M.D. Yale University
% Revised 04-Jan-2019

% Run Decision Tree model for each outcome group

%Three outcomes were defined: all deaths, coronary artery disease deaths,
%and CHD deaths plus myocardial infarctions. The variable numbering 1
%through 3 generallly refers to these three outcomes.

%Inputs: CCTAtables are generated by the B_feature_weight_thresholding.m function.
%Outputs: scores_Tree1 are the linear discriminant scores
%         X_Tree1 and Y_Tree1 define the ROC curve for all deaths
%         AUC_Tree1 is a 1x3 vector containing the AUCs, followed by the
%         pointwise bootstrap confidence interval upper and lower bounds

%This routine is time consuming because of the bootstrapping. To run much faster, 
%disable bootstrapping. No confidence intervals will be returned in that case.
    bootstrapping='off';%'on' or 'off'
    
%select parameters for model
    SplitCriterion='gdi';
    MaxNumSplits=100;
    Surrogate='off';
    
%load data
    load input_data/CCTAtable1.mat
    load input_data/CCTAtable2.mat
    load input_data/CCTAtable3.mat
   
%all deaths
    [response_Tree1,scores_Tree1,X_Tree1,Y_Tree1,T_Tree1,AUC_Tree1] = func_Tree(CCTAtable1,SplitCriterion,MaxNumSplits,Surrogate,bootstrapping);
   
%CHD deaths
    [response_Tree2,scores_Tree2,X_Tree2,Y_Tree2,T_Tree2,AUC_Tree2] = func_Tree(CCTAtable2,SplitCriterion,MaxNumSplits,Surrogate,bootstrapping);
  
%CHD deaths + MI
    [response_Tree3,scores_Tree3,X_Tree3,Y_Tree3,T_Tree3,AUC_Tree3] = func_Tree(CCTAtable3,SplitCriterion,MaxNumSplits,Surrogate,bootstrapping);
     
% Make results table
    DecTree_results_table=table(AUC_Tree1',AUC_Tree2',AUC_Tree3');
    DecTree_results_table.Properties.VariableNames{'Var1'}='DecTree_AUC_alldeaths';
    DecTree_results_table.Properties.VariableNames{'Var2'}='DecTree_AUC_CHDdeaths';
    DecTree_results_table.Properties.VariableNames{'Var3'}='DecTree_AUC_CHDdeathsplusMI';
    DecTree_results_table.Properties.VariableDescriptions={'AUC followed by CI','AUC followed by CI','AUC followed by CI'};
    DecTree_results_table.Properties.UserData.SplitCriterion=SplitCriterion;
    DecTree_results_table.Properties.UserData.MaxNumSplits=MaxNumSplits;
    DecTree_results_table.Properties.UserData.Surrogate=Surrogate;
    DecTree_results_table.Properties.UserData.percentile=parameter.percentile;
    DecTree_results_table.Properties.UserData
    disp(DecTree_results_table)

% Save
    save('results/DecTree_results_table','DecTree_results_table')
    writetable(DecTree_results_table,'results/DecTree_results_table')
    
function [response,scores,X_Tree,Y_Tree,T_Tree,AUC_Tree,validationAccuracy] = func_Tree(trainingData,SplitCriterion,MaxNumSplits,Surrogate,bootstrapping)

    %This code was adapted from the MATLAB Classification Learner App code generator output
   
    % Extract predictors and response
        inputTable = trainingData;
    
    % Split matrices in the input table into vectors
        predictorNames =inputTable.Properties.VariableNames(1:end-1);
        predictors = inputTable(:, predictorNames);
        outcomeName =inputTable.Properties.VariableNames(end);
        response = table2array(inputTable(:, outcomeName));
        
    % Train a classifier
        classificationTree = fitctree(...
            predictors, ...
            response, ...
            'SplitCriterion', SplitCriterion, ...
            'MaxNumSplits', MaxNumSplits, ...
            'Surrogate', Surrogate, ...
            'ClassNames', categorical({'0'; '1'}));

    % Create the result struct with predict function
        splitMatricesInTableFcn = @(t) [t(:,setdiff(t.Properties.VariableNames, {'inputs'})), array2table(table2array(t(:,{'inputs'})), 'VariableNames', {'inputs_1', 'inputs_2', 'inputs_3', 'inputs_4', 'inputs_5', 'inputs_6', 'inputs_7', 'inputs_8', 'inputs_9', 'inputs_10', 'inputs_11', 'inputs_12', 'inputs_13', 'inputs_14', 'inputs_15', 'inputs_16', 'inputs_17', 'inputs_18', 'inputs_19', 'inputs_20', 'inputs_21', 'inputs_22', 'inputs_23', 'inputs_24', 'inputs_25', 'inputs_26', 'inputs_27', 'inputs_28', 'inputs_29', 'inputs_30', 'inputs_31', 'inputs_32'})];
        extractPredictorsFromTableFcn = @(t) t(:, predictorNames);
        predictorExtractionFcn = @(x) extractPredictorsFromTableFcn(splitMatricesInTableFcn(x));
        ensemblePredictFcn = @(x) predict(classificationTree, x);
        trainedClassifier.predictFcn = @(x) ensemblePredictFcn(predictorExtractionFcn(x));

    % Add additional fields to the result struct
        trainedClassifier.RequiredVariables = {'inputs'};
        trainedClassifier.ClassificationEnsemble = classificationTree;
        trainedClassifier.About = 'This struct is a trained model exported from Classification Learner R2017b.';
        trainedClassifier.HowToPredict = sprintf('To make predictions on a new table, T, use: \n  yfit = c.predictFcn(T) \nreplacing ''c'' with the name of the variable that is this struct, e.g. ''trainedModel''. \n \nThe table, T, must contain the variables returned by: \n  c.RequiredVariables \nVariable formats (e.g. matrix/vector, datatype) must match the original training data. \nAdditional variables are ignored. \n \nFor more information, see <a href="matlab:helpview(fullfile(docroot, ''stats'', ''stats.map''), ''appclassification_exportmodeltoworkspace'')">How to predict using an exported model</a>.');

    % Perform cross-validation
        partitionedModel = crossval(trainedClassifier.ClassificationEnsemble, 'KFold', 10);

    % Compute validation predictions
        [~,validationScores] = kfoldPredict(partitionedModel);
        scores=validationScores(:,2);
     
    % Compute validation accuracy
        validationAccuracy = 1 - kfoldLoss(partitionedModel, 'LossFun', 'ClassifError');

    % ROC curves with bootstrap confidence intervals
        if strcmp(bootstrapping,'on')
            [X_Tree,Y_Tree,T_Tree,AUC_Tree]=perfcurve(response,scores,1,'nboot',1000);
        else
            [X_Tree,Y_Tree,T_Tree,AUC_Tree]=perfcurve(response,scores,1);
        end
    
end
