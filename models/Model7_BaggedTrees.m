
% Script for Bagged Trees model
% CCTA data analysis
% Created 07-Apr-2018 Kevin M. Johnson, M.D. Yale University
% Revised 04-Jan-2019

% Run bagged tree model for each outcome group

%Three outcomes were defined: all deaths, coronary artery disease deaths,
%and CHD deaths plus myocardial infarctions. The variable numbering 1
%through 3 generallly refers to these three outcomes.

%Inputs: CCTAtables are generated by the B_threshold_inputs.m function.
%Outputs: scores_BAG1 are the linear discriminant scores
%         X_BAG1 and Y_BAG1 define the ROC curve for all deaths
%         AUC_BAG1 is a 1x3 vector containing the AUCs, followed by the
%         pointwise bootstrap confidence interval upper and lower bounds

%This routine is time consuming. To run much faster, disable bootstrapping. 
%No confidence intervals will be returned in that case.
    bootstrapping='off';%'on' or 'off'
    
%select parameters for model
    Method='Bag';
    MaxNumSplits=10;
    NumLearningCycles=200;
    
%load data
    load input_data/CCTAtable1.mat
    load input_data/CCTAtable2.mat
    load input_data/CCTAtable3.mat

%Temporarily rename variables so we can use same function for all outcomes 
%all deaths
    [response_BAG1,scores_BAG1,X_BAG1,Y_BAG1,T_BAG1,AUC_BAG1] = func_BAG(CCTAtable1,Method,MaxNumSplits,NumLearningCycles,bootstrapping);
   
%CHD deaths
    [response_BAG2,scores_BAG2,X_BAG2,Y_BAG2,T_BAG2,AUC_BAG2] = func_BAG(CCTAtable2,Method,MaxNumSplits,NumLearningCycles,bootstrapping);
  
%CHD deaths + MI
    [response_BAG3,scores_BAG3,X_BAG3,Y_BAG3,T_BAG3,AUC_BAG3] = func_BAG(CCTAtable3,Method,MaxNumSplits,NumLearningCycles,bootstrapping);
     
% Make results table
    BAG_results_table=table(AUC_BAG1',AUC_BAG2',AUC_BAG3');
    BAG_results_table.Properties.VariableNames{'Var1'}='BAG_AUC_alldeaths';
    BAG_results_table.Properties.VariableNames{'Var2'}='BAG_AUC_CHDdeaths';
    BAG_results_table.Properties.VariableNames{'Var3'}='BAG_AUC_CHDdeathsplusMI';
    BAG_results_table.Properties.VariableDescriptions={'AUC followed by CI','AUC followed by CI','AUC followed by CI'};
    BAG_results_table.Properties.UserData.Method=Method;
    BAG_results_table.Properties.UserData.MaxNumSplits=MaxNumSplits;
    BAG_results_table.Properties.UserData.NumLearningCycles=NumLearningCycles;
    BAG_results_table.Properties.UserData.percentile=parameter.percentile;
    BAG_results_table.Properties.UserData
    disp(BAG_results_table)

% Save
    save('results/BAG_results_table','BAG_results_table')
    writetable(BAG_results_table,'results/BAG_results_table')
    
function [response,scores,X_BAG,Y_BAG,T_BAG,AUC_BAG,validationAccuracy] = func_BAG(trainingData,Method,MaxNumSplits,NumLearningCycles,bootstrapping)

    %This code was adapted from the MATLAB Classification Learner App code generator output
   
    % Extract predictors and response
        inputTable = trainingData;
    
    % Split matrices in the input table into vectors
        predictorNames =inputTable.Properties.VariableNames(1:end-1);
        predictors = inputTable(:, predictorNames);
        outcomeName =inputTable.Properties.VariableNames(end);
        response = table2array(inputTable(:, outcomeName));

    % Train a classifier
        template = templateTree('MaxNumSplits', MaxNumSplits);
        classificationEnsemble = fitcensemble(...
            predictors, ...
            response, ...
            'Method', Method, ...
            'NumLearningCycles', NumLearningCycles, ...
            'Learners', template, ...
            'ClassNames', categorical({'0'; '1'}));

    % Create the result struct with predict function
        splitMatricesInTableFcn = @(t) [t(:,setdiff(t.Properties.VariableNames, {'inputs'})), array2table(table2array(t(:,{'inputs'})), 'VariableNames', {'inputs_1', 'inputs_2', 'inputs_3', 'inputs_4', 'inputs_5', 'inputs_6', 'inputs_7', 'inputs_8', 'inputs_9', 'inputs_10', 'inputs_11', 'inputs_12', 'inputs_13', 'inputs_14', 'inputs_15', 'inputs_16', 'inputs_17', 'inputs_18', 'inputs_19', 'inputs_20', 'inputs_21', 'inputs_22', 'inputs_23', 'inputs_24', 'inputs_25', 'inputs_26', 'inputs_27', 'inputs_28', 'inputs_29', 'inputs_30', 'inputs_31', 'inputs_32'})];
        extractPredictorsFromTableFcn = @(t) t(:, predictorNames);
        predictorExtractionFcn = @(x) extractPredictorsFromTableFcn(splitMatricesInTableFcn(x));
        ensemblePredictFcn = @(x) predict(classificationEnsemble, x);
        trainedClassifier.predictFcn = @(x) ensemblePredictFcn(predictorExtractionFcn(x));

    % Add additional fields to the result struct
        trainedClassifier.RequiredVariables = {'inputs'};
        trainedClassifier.ClassificationEnsemble = classificationEnsemble;
        trainedClassifier.About = 'This struct is a trained model exported from Classification Learner R2017b.';
        trainedClassifier.HowToPredict = sprintf('To make predictions on a new table, T, use: \n  yfit = c.predictFcn(T) \nreplacing ''c'' with the name of the variable that is this struct, e.g. ''trainedModel''. \n \nThe table, T, must contain the variables returned by: \n  c.RequiredVariables \nVariable formats (e.g. matrix/vector, datatype) must match the original training data. \nAdditional variables are ignored. \n \nFor more information, see <a href="matlab:helpview(fullfile(docroot, ''stats'', ''stats.map''), ''appclassification_exportmodeltoworkspace'')">How to predict using an exported model</a>.');

    % Perform cross-validation
        partitionedModel = crossval(trainedClassifier.ClassificationEnsemble, 'KFold', 10);

    % Compute validation predictions
        [~,validationScores] = kfoldPredict(partitionedModel);
        scores=validationScores(:,2);
     
    % Compute validation accuracy
        validationAccuracy = 1 - kfoldLoss(partitionedModel, 'LossFun', 'ClassifError');

    % ROC curves with bootstrap confidence intervals
        if strcmp(bootstrapping,'on')
            [X_BAG,Y_BAG,T_BAG,AUC_BAG]=perfcurve(response,scores,1,'nboot',1000);
        else
            [X_BAG,Y_BAG,T_BAG,AUC_BAG]=perfcurve(response,scores,1);
        end
    
end
